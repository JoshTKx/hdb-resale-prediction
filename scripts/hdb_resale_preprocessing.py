# -*- coding: utf-8 -*-
"""HDB_resale_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8oL6HIzQbI46mP5bTIaWtkho3YHLVZ0

#HDB Resale Price Data Preprocessing

##Preparing Data for Neural Network Training
This notebook focuses on transforming raw HDB resale data into clean, encoded features ready for PyTorch model training.
##Project Context
Based on comprehensive exploratory analysis of 213,883 HDB transactions, we identified complex non-linear relationships that justify a neural network approach. This preprocessing pipeline prepares the data for PyTorch implementation.

##1. Data Loading and Setup
Loading the raw dataset and setting up our preprocessing environment.
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
from google.colab import drive
drive.mount('/content/drive')

try:
  df = pd.read_csv('/content/drive/MyDrive/HDB_resale_project/hdb_resale_price_2017-present.csv')
  print("File successfully loaded")
  display(df.head())
  print(f"Loaded {len(df)} records")
  print("Data shape:", df.shape)
except FileNotFoundError:
  print("Error: File not found. Please make sure the file name is correct and the file is uploaded.")
except Exception as e:
  print(f"An error occurred: {e}")

"""##Data Loading Results:

- Successfully loaded 213,883 HDB resale transactions
- All 11 original variables present with no missing values
- Dataset spans from 2017 to 2025, providing comprehensive market coverage

##2. Feature Engineering Pipeline
Converting raw data into meaningful features that capture market dynamics and property characteristics.

###**Temporal Feature Extraction**
Converting date strings to meaningful temporal features that capture market cycles and seasonality.
"""

# Extract temporal features
df['month_datetime'] = pd.to_datetime(df['month'])
df['year'] = df['month_datetime'].dt.year
df['month_num'] = df['month_datetime'].dt.month

"""###**Remaining Lease Conversion**
Converting text-based lease information ("61 years 04 months") to numerical values for model input.
"""

# Convert remaining lease to months (using your function)
def convert_to_months(text):
    text = text.split()
    months = 0
    for i, word in enumerate(text):
        if word in ["years", "year"]:
            months += int(text[i-1]) * 12
        elif word in ["months", "month"]:
            months += int(text[i-1])
    return months

df['remaining_lease_months'] = df['remaining_lease'].apply(convert_to_months)

"""###**Building Age Calculation**
Computing property age at time of sale to capture depreciation effects.
"""

# Calculate building age at time of sale
df['building_age'] = df['year'] - df['lease_commence_date']

"""##Feature Engineering Results:

- **Temporal features**: Successfully extracted year and month components for seasonality analysis
- **Remaining lease**: Converted complex text format to numerical months (400-1200 range)
- **Building age**: Calculated property age at time of sale for depreciation modeling
- **Feature validation**: All engineered features show logical ranges and distributions

##3. Categorical Variable Encoding

###**Label Encoding for Neural Network Compatibility**
Converting categorical text variables to integer indices required for PyTorch embedding layers.
"""

# Categorical features for embeddings
categorical_features = ['town', 'flat_type', 'storey_range', 'flat_model']

# Target
target = 'resale_price'

# Create label encoders for categorical features
encoders = {}
for feature in categorical_features:
    encoders[feature] = LabelEncoder()
    df[feature + '_encoded'] = encoders[feature].fit_transform(df[feature])
    print(f"{feature}: {df[feature].nunique()} unique values -> encoded as 0-{df[feature].nunique()-1}")

# Check the encoding worked
print("\nEncoding verification:")
for feature in categorical_features:
    print(f"{feature} range: {df[feature + '_encoded'].min()} to {df[feature + '_encoded'].max()}")

"""###**Encoding Results:**

- **town:** 26 categories → 0-25 encoding
- **flat_type:** 7 categories → 0-6 encoding
- **storey_range:** 17 categories → 0-16 encoding
- **flat_model:** 21 categories → 0-20 encoding
- **Validation:** All encodings properly mapped with expected ranges

**Embedding Strategy:** These encoded integers will feed into embedding layers that learn dense vector representations for each category during training.

##4. Time-Based Data Splitting
"""

# Sort by date for time-based split
df_sorted = df.sort_values('month_datetime')

# Split by year - what years do you have in your data?
print("Years available:", sorted(df_sorted['year'].unique()))
print("Transactions per year:")
print(df_sorted['year'].value_counts().sort_index())

"""###**Chronological Split Strategy**
Using time-based splits to simulate real-world forecasting scenarios and prevent data leakage.
"""

# Create the splits
train_data = df_sorted[df_sorted['year'] <= 2022].copy()
val_data = df_sorted[df_sorted['year'] == 2023].copy()
test_data = df_sorted[df_sorted['year'] == 2024].copy()  # Excluding 2025 partial data

print("Split sizes:")
print(f"Training: {len(train_data)} records (2017-2022)")
print(f"Validation: {len(val_data)} records (2023)")
print(f"Testing: {len(test_data)} records (2024)")
print(f"Total used: {len(train_data) + len(val_data) + len(test_data)} / {len(df_sorted)}")

"""###**Split Results:**

- **Training Set:** 143,396 records (2017-2022) - Historical data for learning
- **Validation Set:** 25,754 records (2023) - Model selection and hyperparameter tuning
- **Test Set:** 27,833 records (2024) - Final performance evaluation
- **Excluded:** 16,900 records (2025 partial year)

**Split Rationale:** Chronological splits ensure our model learns from past data to predict future prices, mimicking real deployment scenarios.

##5. Continuous Feature Normalization
"""

# Continuous features
continuous_features = ['remaining_lease_months', 'floor_area_sqm', 'year', 'month_num', 'building_age']
scaled_features = [f'{col}_scaled' for col in continuous_features]

"""###**Standardization for Neural Network Training**
Scaling numerical features to mean=0, std=1 to ensure balanced gradient updates during training.
"""

# Initialize scaler
scaler = StandardScaler()

# Create normalized versions of continuous features
scaler.fit(train_data[continuous_features])
train_data[scaled_features] = scaler.transform(train_data[continuous_features])
val_data[scaled_features] = scaler.transform(val_data[continuous_features])
test_data[scaled_features] = scaler.transform(test_data[continuous_features])


print("Continuous features scaling after splitting:")
print(f"Training data mean:\n{train_data[scaled_features].mean().round(3)}")
print(f"Training data std:\n{train_data[scaled_features].std().round(3)}")
print("-" * 30)
print(f"Validation data mean:\n{val_data[scaled_features].mean().round(3)}")
print(f"Validation data std:\n{val_data[scaled_features].std().round(3)}")
print("-" * 30)
print(f"Testing data mean:\n{test_data[scaled_features].mean().round(3)}")
print(f"Testing data std:\n{test_data[scaled_features].std().round(3)}")

"""###**Normalization Results:**
All continuous features successfully standardized:

- **remaining_lease_months:** Primary price driver, now properly scaled
- **floor_area_sqm:** Size effects normalized across flat types
- **year/month:** Temporal features scaled for consistent neural network input
- **building_age:** Age effects standardized for balanced training

**Quality Check:** All scaled features achieve mean≈0, std≈1 as required for optimal neural network performance.

##6. Final Data Preparation for PyTorch

###**Feature Array Extraction**
Converting processed DataFrame into numpy arrays formatted for PyTorch tensor conversion.
"""

# Define our final feature lists (using encoded categoricals and scaled continuous)
categorical_encoded = ['town_encoded', 'flat_type_encoded', 'storey_range_encoded', 'flat_model_encoded']
continuous_scaled = ['remaining_lease_months_scaled', 'floor_area_sqm_scaled', 'year_scaled', 'month_num_scaled', 'building_age_scaled']

# Function to extract features and target
def prepare_features(data):
    # Categorical features (integers for embeddings)
    cat_features = data[categorical_encoded].values.astype(int)

    # Continuous features (floats, already scaled)
    cont_features = data[continuous_scaled].values.astype(float)

    # Target variable
    target = data['resale_price'].values.astype(float)

    return cat_features, cont_features, target

# Prepare all splits
train_cat, train_cont, train_target = prepare_features(train_data)
val_cat, val_cont, val_target = prepare_features(val_data)
test_cat, test_cont, test_target = prepare_features(test_data)

# Save to drive
base_path = '/content/drive/My Drive/HDB_resale_project/preprocessed_data/'
os.makedirs(f'{base_path}train', exist_ok=True)
os.makedirs(f'{base_path}val', exist_ok=True)
os.makedirs(f'{base_path}test', exist_ok=True)

# Save training arrays
np.save(f'{base_path}train/train_cat.npy', train_cat)
np.save(f'{base_path}train/train_cont.npy', train_cont)
np.save(f'{base_path}train/train_target.npy', train_target)

# Save validation arrays
np.save(f'{base_path}val/val_cat.npy', val_cat)
np.save(f'{base_path}val/val_cont.npy', val_cont)
np.save(f'{base_path}val/val_target.npy', val_target)

# Save test arrays
np.save(f'{base_path}test/test_cat.npy', test_cat)
np.save(f'{base_path}test/test_cont.npy', test_cont)
np.save(f'{base_path}test/test_target.npy', test_target)

print("Feature shapes:")
print(f"Categorical: {train_cat.shape}")
print(f"Continuous: {train_cont.shape}")
print(f"Target: {train_target.shape}")

"""###**Data Preparation Summary:**

- **Categorical Features:** (143,396, 4) array ready for embedding layers
- **Continuous Features:** (143,396, 5) array with normalized values
- **Target Variable:** (143,396,) price array for regression training
- **Data Quality:** All arrays properly shaped and typed for neural network input

##Preprocessing Pipeline Complete

###**Summary of Transformations:**

1. **Raw data loaded** and validated (213,883 transactions)
2. **Temporal features engineered** from date strings
3. **Text parsing completed** for remaining lease conversion
4. **Categorical encoding applied** for embedding layer compatibility
5. **Feature normalization completed** for neural network training
6. **Time-based splitting implemented** for realistic evaluation
7. **Data arrays prepared** for PyTorch tensor conversion

**Next Phase:** Ready for neural network architecture design and model training implementation.
"""

# Create sample dataset for GitHub (first 1000 rows)
sample_size = 1000
df_sample = df.head(sample_size).copy()

# Save sample to CSV
sample_path = '/content/drive/MyDrive/HDB_resale_project/sample_hdb_data.csv'
df_sample.to_csv(sample_path, index=False)

print(f"Sample dataset created: {sample_size} rows")
print(f"Saved to: {sample_path}")
print("\nSample data preview:")
print(df_sample.head())
print(f"\nSample covers:")
print(f"- Price range: ${df_sample['resale_price'].min():,.0f} to ${df_sample['resale_price'].max():,.0f}")
print(f"- Time period: {df_sample['month'].min()} to {df_sample['month'].max()}")
print(f"- Towns: {df_sample['town'].nunique()} unique ({', '.join(sorted(df_sample['town'].unique())[:5])}...)")
print(f"- Flat types: {df_sample['flat_type'].nunique()} unique")



print("\n" + "="*50)
print("SAVING PREPROCESSING PARAMETERS FOR INFERENCE")
print("="*50)

# 1. Save scaler parameters
scaler_params = {}
continuous_feature_names = ['remaining_lease_months', 'floor_area_sqm', 'year', 'month_num', 'building_age']

for i, feature in enumerate(continuous_feature_names):
    scaler_params[feature] = {
        'mean': float(scaler.mean_[i]),
        'std': float(scaler.scale_[i])
    }

print("Scaler parameters:")
for feature, params in scaler_params.items():
    print(f"  {feature}: mean={params['mean']:.2f}, std={params['std']:.2f}")

# 2. Save encoder mappings
encoder_mappings = {}
for feature in categorical_features:
    classes = encoders[feature].classes_
    encoder_mappings[feature] = {str(cls): int(idx) for idx, cls in enumerate(classes)}

print(f"\nEncoder mappings created for {len(encoder_mappings)} categorical features:")
for feature, mapping in encoder_mappings.items():
    print(f"  {feature}: {len(mapping)} categories (0 to {len(mapping)-1})")

# 3. Save to files
import json
params_dir = '/content/drive/My Drive/HDB_resale_project/preprocessed_data/'

with open(f'{params_dir}scaler_params.json', 'w') as f:
    json.dump(scaler_params, f, indent=2)

with open(f'{params_dir}encoder_mappings.json', 'w') as f:
    json.dump(encoder_mappings, f, indent=2)

print(f"\nParameters saved to:")
print(f"  - {params_dir}scaler_params.json")
print(f"  - {params_dir}encoder_mappings.json")

# 4. Display actual town mappings for verification
print(f"\nActual town mappings (first 10):")
town_mapping = encoder_mappings['town']
for i, (town, encoded) in enumerate(list(town_mapping.items())[:10]):
    print(f"  '{town}': {encoded}")
if len(town_mapping) > 10:
    print(f"  ... and {len(town_mapping)-10} more towns")